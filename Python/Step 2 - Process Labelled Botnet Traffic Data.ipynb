{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Labelled Botnet Traffic Data\n",
    "Here you can find Python code to process Labelled Botnet Traffic Data\n",
    "\n",
    "The code shows you how to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x) ##remove scientific notation\n",
    "import numpy as np\n",
    "from time import time\n",
    "from sklearn.neighbors import LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outliers_fraction = 0.5 #we tell the outlier detection algo to remove 50% of the samples\n",
    "rng = np.random.RandomState(442) #random seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 2.1: Load the labelled Dataset\n",
    "Notice that we have already labelled the dataset according to the guidelines provided on the ISCX web-page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('script started!',flush=True)\n",
    "t0 = time()\n",
    "data = pd.read_csv('ISCX_ISCX_Botnet-Training-LABELLED.pcap.csv')\n",
    "t1 = time()\n",
    "print('data loaded in %f'%(t1-t0),flush=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.2: Randomly Choose a Subset from the Big Dataset\n",
    "* This is only for demonstration purposes\n",
    "* The original dataset is **too big** and it takes a long time to finish the following steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets randomly take 5000 samples to make things quick\n",
    "#in real application we must use all the data\n",
    "data = data.sample(n=5000,random_state=4646465)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can see the data still have the source port and disination port (without one hot encoding)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.3: Missing Value Imputation\n",
    "* The CSV file resulting from Flow Generator can have missing values\n",
    "* Here we replace them with the **average** (i.e. the mean)\n",
    "* We go through columns one by one and do the impuation using the mean of each column\n",
    "* We can use the **median** instead of the mean\n",
    "* We can explore other missing value imputation techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we apply missing value imputation\n",
    "#replace any missing value in a column with the mean of that column\n",
    "t0 = time()\n",
    "data.replace('?', np.NaN,inplace=True)\n",
    "print('symbol ? replaced with NaN',flush=True)\n",
    "for c in data.columns:\n",
    "    if c != 'BotNet_Label':\n",
    "        #print(c)\n",
    "        data[c] = pd.to_numeric(data[c], errors='coerce')\n",
    "        data[c] = data[c].replace(np.NaN,data[c].mean())#median can be used instead of mean!\n",
    "#save data so we can use it later\n",
    "data.to_csv('no_nans.csv',index=False)\n",
    "t1 = time()\n",
    "print('NaN values replaced with mean in %f'%(t1-t0),flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.4: Perform one hot encoding to convert source port, dest port and protocol columns to binary numeric columns\n",
    "* Although these columns contain numbers, they are in reality categorical \n",
    "* If we leave them as numbers, the machine learning algorithms will use them as numeric columns and results will not be reliable\n",
    "* This is why we perform one-hot encoding to transform them into numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform one hot encoding to convert source port, dest port and protocol columns to \n",
    "# binary numeric columns\n",
    "# because these columns are categorical and not numerical\n",
    "# if we leave them as they are, the algorithms will treat them as numerical\n",
    "t0 = time()\n",
    "#create dummy variables for source port, dest port and protocol\n",
    "src_port_df = pd.get_dummies(data['Source Port'])\n",
    "dest_port_df = pd.get_dummies(data['Destination Port'])\n",
    "protocol_df = pd.get_dummies(data['Protocol'])\n",
    "#put all the data together in one data frame\n",
    "data = pd.concat([src_port_df,dest_port_df,protocol_df,data],axis=1)\n",
    "#remove source port, dest port and protocol because we now\n",
    "#have one hot encoding for them (dummy variables)\n",
    "data.drop(['Source Port', 'Destination Port', 'Protocol'],inplace=True,axis=1)\n",
    "\n",
    "#because port values are integers, they will appear as integers in column names\n",
    "#this step converts them to strings\n",
    "data.columns = data.columns.astype(str)\n",
    "\n",
    "t1 = time()\n",
    "print('One hot encoding finished in %f'%(t1-t0),flush=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can notice that the one hot encoding applied and each source port and destination port have seperate coloumns\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.5: Remove Highly Correlated features\n",
    "* I read the paper about ISCX Flow Generator and there the authors there explain the features that this tool extracts from PCAP files\n",
    "* Some of these features are highly correlated and it is better to remove them because they can affect the performance of machine learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we remove highly correlated features and prepare the data for outlier detection\n",
    "t0 = time()\n",
    "\n",
    "Y = data['BotNet_Label']\n",
    "data.drop('BotNet_Label', axis=1, inplace=True)\n",
    "\n",
    "#remove single quote from column names if they exist\n",
    "rm_quote = lambda x: x.replace('\\'', '')\n",
    "cols = data.columns\n",
    "data.columns = [rm_quote(x) for x in cols]\n",
    "\n",
    "#remove highly correlated columns\n",
    "data.drop(['Flow IAT Max','Flow IAT Min','Fwd IAT Mean','Fwd IAT Std','Fwd IAT Max','Fwd IAT Min','Bwd IAT Max','Bwd IAT Min','Active Max','Active Min','Idle Mean','Idle Max','Idle Min'],axis=1, inplace=True)\n",
    "\n",
    "\n",
    "t1 = time()\n",
    "print('X (data) and Y matrices prepared in %f'%(t1-t0),flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.6: Outlier Detection and Remval\n",
    "* Here we detect and remove outliers because they usually influence models and affect results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model that performs outlier detection\n",
    "\n",
    "clf = LocalOutlierFactor(n_neighbors=50, contamination=outliers_fraction,n_jobs = 60)\n",
    "y_pred = clf.fit_predict(data)#only for LocalOutlierFactor\n",
    "\n",
    "data['Outlier'] = y_pred\n",
    "data['BotNet_Label'] = Y\n",
    "\n",
    "outlier_mask = data['Outlier'].isin([-1])\n",
    "\n",
    "print('To apply mask and removed outliters',flush=True)\n",
    "data = data.loc[~outlier_mask]\n",
    "data.drop('Outlier', axis=1, inplace=True)\n",
    "print('data to be saved',flush=True)\n",
    "data.to_csv('no_outliers.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.7: Split the Big Dataset into smaller sub-datasets According to Label\n",
    "* We need to have a separate dataset for each Botnet so we can perform transfer learning and other ML algorithms\n",
    "* Later we can add **Normal** data to each Botnet dataset\n",
    "* We must make sure Normal data we add to each Botnet dataset is **non-overlapping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save subdatasets\n",
    "labels = list(data['BotNet_Label'].unique())\n",
    "for label in labels:\n",
    "    tlbl = label.replace(\" \", \"_\")#if label has space replace it with _\n",
    "    tdata = data[data['BotNet_Label']==label]\n",
    "    tdata.to_csv('Subdatasets/ISCX_Testing_'+tlbl+'.csv',index=False)\n",
    "    print('Done: ',label,len(tdata))\n",
    "\n",
    "print('all done', flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
